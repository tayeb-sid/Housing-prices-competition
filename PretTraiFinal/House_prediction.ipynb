{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DGA5OlSh4dr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import randint, uniform\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, PredefinedSplit, GridSearchCV\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgz71Hegjpp7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "##############################################################################\n",
        "# 1) Define global resources so we can reuse the same objects for train & test\n",
        "##############################################################################\n",
        "\n",
        "# --- Ordinal mapping for your ordinal columns ---\n",
        "ordinal_mapping = {\n",
        "    \"GarageQual\":    {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
        "    \"Fence\":         {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\": 2, \"MnWw\": 1, \"NA\": 0},\n",
        "    \"GarageFinish\":  {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"NA\": 0},\n",
        "    \"KitchenQual\":   {\"Ex\": 4, \"Gd\": 3, \"TA\": 2, \"Fa\": 1, \"Po\": 0},\n",
        "    \"GarageCond\":    {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
        "    \"HeatingQC\":     {\"Ex\": 4, \"Gd\": 3, \"TA\": 2, \"Fa\": 1, \"Po\": 0},\n",
        "    \"ExterQual\":     {\"Ex\": 4, \"Gd\": 3, \"TA\": 2, \"Fa\": 1, \"Po\": 0},\n",
        "    \"BsmtCond\":      {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
        "    \"LandSlope\":     {\"Gtl\": 2, \"Mod\": 1, \"Sev\": 0},\n",
        "    \"ExterCond\":     {\"Ex\": 4, \"Gd\": 3, \"TA\": 2, \"Fa\": 1, \"Po\": 0},\n",
        "    \"BsmtExposure\":  {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"NA\": 0},\n",
        "    \"PavedDrive\":    {\"Y\": 2, \"P\": 1, \"N\": 0},\n",
        "    \"BsmtQual\":      {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
        "    \"LotShape\":      {\"Reg\": 3, \"IR1\": 2, \"IR2\": 1, \"IR3\": 0},\n",
        "    \"BsmtFinType2\":  {\"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1, \"NA\": 0},\n",
        "    \"BsmtFinType1\":  {\"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1, \"NA\": 0},\n",
        "    \"FireplaceQu\":   {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
        "    \"Utilities\":     {\"AllPub\": 3, \"NoSewr\": 2, \"NoSeWa\": 1, \"ELO\": 0},\n",
        "    \"Functional\":    {\"Typ\": 7, \"Min1\": 6, \"Min2\": 5, \"Mod\": 4, \"Maj1\": 3,\n",
        "                      \"Maj2\": 2, \"Sev\": 1, \"Sal\": 0},\n",
        "    \"PoolQC\":        {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
        "}\n",
        "# Extract the list of ordinal columns from that mapping\n",
        "ordinal_cols = list(ordinal_mapping.keys())\n",
        "\n",
        "# --- Nominal features (for One-Hot Encoding) ---\n",
        "nominal_features = [\n",
        "    \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\",\n",
        "    \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\",\n",
        "    \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\",\n",
        "    \"CentralAir\", \"Electrical\", \"GarageType\", \"MiscFeature\", \"SaleType\",\n",
        "    \"SaleCondition\"\n",
        "]\n",
        "\n",
        "# Create ONE global OrdinalEncoder and ONE global OneHotEncoder.\n",
        "# We'll fit them on train, then reuse them to transform test.\n",
        "ordinal_encoder = OrdinalEncoder(\n",
        "    categories=[list(ordinal_mapping[col].keys()) for col in ordinal_cols],\n",
        "    handle_unknown=\"use_encoded_value\",\n",
        "    unknown_value=-1\n",
        ")\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "##############################################################################\n",
        "# 2) Main preprocessing function\n",
        "##############################################################################\n",
        "# def preprocess_data(df, is_train=True):\n",
        "#     \"\"\"\n",
        "#     Preprocess a housing DataFrame.\n",
        "#     If is_train=True, drops outliers and fits the encoders.\n",
        "#     If is_train=False, only transforms using the fitted encoders (NO outlier removal).\n",
        "\n",
        "#     Returns:\n",
        "#       (X, y) if is_train=True\n",
        "#       X if is_train=False\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Make a copy so we don't overwrite the original data\n",
        "#     df = df.copy()\n",
        "\n",
        "#     # If this is training data, grab the target 'SalePrice'\n",
        "#     # and remove it from the DataFrame.\n",
        "#     if is_train:\n",
        "#         y = df['SalePrice']\n",
        "#         df.drop('SalePrice', axis=1, inplace=True)\n",
        "#     else:\n",
        "#         y = None  # no target in test set\n",
        "\n",
        "#     # 1) Convert certain columns to numeric if they exist\n",
        "#     cols_to_numeric = [\n",
        "#         \"LotFrontage\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\n",
        "#         \"TotalBsmtSF\", \"BsmtFullBath\", \"GarageCars\", \"BsmtHalfBath\", \"GarageYrBlt\"\n",
        "#     ]\n",
        "#     for col in cols_to_numeric:\n",
        "#         if col in df.columns:\n",
        "#             df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "#     # 2) Fill numeric columns with 0, categorical with \"NA\"\n",
        "#     numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "#     df[numeric_cols] = df[numeric_cols].fillna(0)\n",
        "\n",
        "#     # Identify any columns of object/string dtype\n",
        "#     cat_cols = df.select_dtypes(include=['object']).columns\n",
        "#     for cat_col in cat_cols:\n",
        "#         df[cat_col] = df[cat_col].fillna(\"NA\")\n",
        "\n",
        "#     # 3) Feature engineering examples (abbreviated from your original code)\n",
        "#     #    Only do these if the columns exist in df.\n",
        "#     if set([\"FullBath\",\"HalfBath\",\"BsmtFullBath\",\"BsmtHalfBath\"]).issubset(df.columns):\n",
        "#         df[\"TotalBath\"] = (\n",
        "#             df[\"FullBath\"] +\n",
        "#             0.5 * df[\"HalfBath\"] +\n",
        "#             df[\"BsmtFullBath\"] +\n",
        "#             0.5 * df[\"BsmtHalfBath\"]\n",
        "#         )\n",
        "#     else:\n",
        "#         # If any are missing, you can either create them or skip\n",
        "#         df[\"TotalBath\"] = 0\n",
        "\n",
        "#     if \"GarageYrBlt\" in df.columns:\n",
        "#         df[\"Has_garage\"] = df[\"GarageYrBlt\"].notnull().astype(int)\n",
        "\n",
        "#     if set([\"YrSold\",\"YearBuilt\"]).issubset(df.columns):\n",
        "#         df[\"House_Age\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n",
        "\n",
        "#     if set([\"YearBuilt\",\"YearRemodAdd\"]).issubset(df.columns):\n",
        "#         df[\"Is_Remodeled\"] = (df[\"YearBuilt\"] != df[\"YearRemodAdd\"]).astype(int)\n",
        "\n",
        "#     # ... etc. for other features from your original code ...\n",
        "#     # e.g., if you want \"House_Age2\", \"GrLivArea\" / \"LotArea\" ratio, etc.\n",
        "\n",
        "#     # 4) Outlier removal: only do this on TRAIN.\n",
        "#     #    Make sure to drop rows from y as well to keep them aligned.\n",
        "#     if is_train:\n",
        "#         # (a) LotFrontage > 200\n",
        "#         if \"LotFrontage\" in df.columns:\n",
        "#             idx = df[df[\"LotFrontage\"] > 200].index\n",
        "#             df.drop(idx, inplace=True)\n",
        "#             y.drop(idx, inplace=True)\n",
        "\n",
        "#         # (b) LotArea > 100000\n",
        "#         if \"LotArea\" in df.columns:\n",
        "#             idx = df[df[\"LotArea\"] > 100000].index\n",
        "#             df.drop(idx, inplace=True)\n",
        "#             y.drop(idx, inplace=True)\n",
        "\n",
        "#         # (c) BsmtFinSF1 > 4000\n",
        "#         if \"BsmtFinSF1\" in df.columns:\n",
        "#             idx = df[df[\"BsmtFinSF1\"] > 4000].index\n",
        "#             df.drop(idx, inplace=True)\n",
        "#             y.drop(idx, inplace=True)\n",
        "\n",
        "#         # (d) TotalBsmtSF > 6000\n",
        "#         if \"TotalBsmtSF\" in df.columns:\n",
        "#             idx = df[df[\"TotalBsmtSF\"] > 6000].index\n",
        "#             df.drop(idx, inplace=True)\n",
        "#             y.drop(idx, inplace=True)\n",
        "\n",
        "#         # (e) 1stFlrSF > 4000\n",
        "#         if \"1stFlrSF\" in df.columns:\n",
        "#             idx = df[df[\"1stFlrSF\"] > 4000].index\n",
        "#             df.drop(idx, inplace=True)\n",
        "#             y.drop(idx, inplace=True)\n",
        "\n",
        "#         # (f) GrLivArea > 4000 & y < 300000\n",
        "#         if \"GrLivArea\" in df.columns:\n",
        "#             idx = df[(df[\"GrLivArea\"] > 4000) & (y < 300000)].index\n",
        "#             df.drop(idx, inplace=True)\n",
        "#             y.drop(idx, inplace=True)\n",
        "\n",
        "#         # (g) LowQualFinSF > 550\n",
        "#         if \"LowQualFinSF\" in df.columns:\n",
        "#             idx = df[df[\"LowQualFinSF\"] > 550].index\n",
        "#             df.drop(idx, inplace=True)\n",
        "#             y.drop(idx, inplace=True)\n",
        "\n",
        "#     # 5) Ordinal Encoding for the columns in ordinal_mapping\n",
        "#     #    We'll only encode columns that actually exist in df.\n",
        "#     existing_ordinal = [col for col in ordinal_cols if col in df.columns]\n",
        "\n",
        "#     # Clean up whitespace if any\n",
        "#     for col in existing_ordinal:\n",
        "#         df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "#     # If train: fit_transform\n",
        "#     # If test:  transform only\n",
        "#     if len(existing_ordinal) > 0:\n",
        "#         if is_train:\n",
        "#             df[existing_ordinal] = ordinal_encoder.fit_transform(df[existing_ordinal])\n",
        "#         else:\n",
        "#             df[existing_ordinal] = ordinal_encoder.transform(df[existing_ordinal])\n",
        "#         # Replace unknown encodings (-1) with 0\n",
        "#         df.replace(-1, 0, inplace=True)\n",
        "\n",
        "#     # 6) One-Hot Encoding for nominal features\n",
        "#     existing_nominal = [col for col in nominal_features if col in df.columns]\n",
        "\n",
        "#     if len(existing_nominal) > 0:\n",
        "#         if is_train:\n",
        "#             # Fit on train\n",
        "#             encoded = one_hot_encoder.fit_transform(df[existing_nominal])\n",
        "#         else:\n",
        "#             # Transform test\n",
        "#             encoded = one_hot_encoder.transform(df[existing_nominal])\n",
        "\n",
        "#         # Convert to DataFrame\n",
        "#         encoded_df = pd.DataFrame(\n",
        "#             encoded,\n",
        "#             columns=one_hot_encoder.get_feature_names_out(existing_nominal),\n",
        "#             index=df.index\n",
        "#         )\n",
        "#         # Drop original nominal cols, then join\n",
        "#         df.drop(columns=existing_nominal, inplace=True)\n",
        "#         df = df.join(encoded_df)\n",
        "\n",
        "#     # Return final results\n",
        "#     if is_train:\n",
        "#         return df, y\n",
        "#     else:\n",
        "#         return df\n",
        "\n",
        "def preprocess_data(df, is_train=True):\n",
        "    \"\"\"\n",
        "    Preprocess a housing DataFrame.\n",
        "    If is_train=True, drops outliers and fits the encoders.\n",
        "    If is_train=False, only transforms using the fitted encoders (NO outlier removal).\n",
        "\n",
        "    Returns:\n",
        "      (X, y) if is_train=True\n",
        "      X if is_train=False\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a copy so we don't overwrite the original data\n",
        "    df = df.copy()\n",
        "\n",
        "    # If this is training data, grab the target 'SalePrice'\n",
        "    # and remove it from the DataFrame.\n",
        "    if is_train:\n",
        "        y = df['SalePrice']\n",
        "        df.drop('SalePrice', axis=1, inplace=True)\n",
        "    else:\n",
        "        y = None  # no target in test set\n",
        "\n",
        "    # 1) Convert certain columns to numeric if they exist\n",
        "    cols_to_numeric = [\n",
        "        \"LotFrontage\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\n",
        "        \"TotalBsmtSF\", \"BsmtFullBath\", \"GarageCars\", \"BsmtHalfBath\", \"GarageYrBlt\"\n",
        "    ]\n",
        "    for col in cols_to_numeric:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # 2) Fill numeric columns with 0, categorical with \"NA\"\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
        "\n",
        "    # Identify any columns of object/string dtype\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns\n",
        "    for cat_col in cat_cols:\n",
        "        df[cat_col] = df[cat_col].fillna(\"NA\")\n",
        "\n",
        "    # 3) Feature engineering examples\n",
        "    # Existing Feature Engineering\n",
        "    if set([\"FullBath\",\"HalfBath\",\"BsmtFullBath\",\"BsmtHalfBath\"]).issubset(df.columns):\n",
        "        df[\"TotalBath\"] = (\n",
        "            df[\"FullBath\"] +\n",
        "            0.5 * df[\"HalfBath\"] +\n",
        "            df[\"BsmtFullBath\"] +\n",
        "            0.5 * df[\"BsmtHalfBath\"]\n",
        "        )\n",
        "    else:\n",
        "        df[\"TotalBath\"] = 0\n",
        "\n",
        "    if \"GarageYrBlt\" in df.columns:\n",
        "        df[\"Has_garage\"] = df[\"GarageYrBlt\"].notnull().astype(int)\n",
        "\n",
        "    if set([\"YrSold\",\"YearBuilt\"]).issubset(df.columns):\n",
        "        df[\"House_Age\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n",
        "\n",
        "    if set([\"YearBuilt\",\"YearRemodAdd\"]).issubset(df.columns):\n",
        "        df[\"Is_Remodeled\"] = (df[\"YearBuilt\"] != df[\"YearRemodAdd\"]).astype(int)\n",
        "\n",
        "    # New Feature Engineering (your provided list)\n",
        "    if set([\"Street\", \"Alley\", \"MasVnrType\", \"GarageType\", \"MiscFeature\",\n",
        "            \"BsmtQual\", \"FireplaceQu\", \"PoolQC\", \"Fence\", \"CentralAir\"]).issubset(df.columns):\n",
        "        # Count missing values in the selected columns and add penalties if certain conditions are met\n",
        "        df[\"Lack_of_feature_index\"] = df[[\"Street\", \"Alley\", \"MasVnrType\", \"GarageType\", \"MiscFeature\",\n",
        "                                           \"BsmtQual\", \"FireplaceQu\", \"PoolQC\", \"Fence\"]].isnull().sum(axis=1) + \\\n",
        "                                       (df[\"MasVnrType\"] == 'None') + (df[\"CentralAir\"] == 'No')\n",
        "    else:\n",
        "        df[\"Lack_of_feature_index\"] = 0\n",
        "\n",
        "    for col in [\"PoolQC\", \"MiscFeature\", \"Fence\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.NA\n",
        "\n",
        "    df[\"MiscFeatureExtended\"] = (\n",
        "        df[\"PoolQC\"].notnull().astype(int) +\n",
        "        df[\"MiscFeature\"].notnull().astype(int) +\n",
        "        df[\"Fence\"].notnull().astype(int)\n",
        "    ).astype(int)\n",
        "\n",
        "    if \"Alley\" in df.columns:\n",
        "        df[\"Has_Alley\"] = df[\"Alley\"].notnull().astype(int)\n",
        "    else:\n",
        "        df[\"Has_Alley\"] = 0\n",
        "\n",
        "    if set([\"GrLivArea\", \"LotArea\"]).issubset(df.columns):\n",
        "        df[\"Lot_occupation\"] = df[\"GrLivArea\"] / df[\"LotArea\"]\n",
        "    else:\n",
        "        df[\"Lot_occupation\"] = 0\n",
        "\n",
        "    if set([\"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\"]).issubset(df.columns):\n",
        "        df[\"Number_of_floors\"] = (\n",
        "            (df[\"TotalBsmtSF\"] != 0).astype(int) +\n",
        "            (df[\"1stFlrSF\"] != 0).astype(int) +\n",
        "            (df[\"2ndFlrSF\"] != 0).astype(int)\n",
        "        )\n",
        "    else:\n",
        "        df[\"Number_of_floors\"] = 0\n",
        "\n",
        "    if set([\"GrLivArea\", \"TotalBsmtSF\"]).issubset(df.columns):\n",
        "        df['Total_Close_Live_Area'] = df['GrLivArea'] + df['TotalBsmtSF']\n",
        "    else:\n",
        "        df['Total_Close_Live_Area'] = 0\n",
        "\n",
        "    if set([\"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]).issubset(df.columns):\n",
        "        df['Outside_live_area'] = (\n",
        "            df['WoodDeckSF'] +\n",
        "            df['OpenPorchSF'] +\n",
        "            df['EnclosedPorch'] +\n",
        "            df['3SsnPorch'] +\n",
        "            df['ScreenPorch']\n",
        "        )\n",
        "    else:\n",
        "        df['Outside_live_area'] = 0\n",
        "\n",
        "    df['Total_usable_area'] = df.get('Total_Close_Live_Area', 0) + df.get('Outside_live_area', 0)\n",
        "\n",
        "    if \"OverallQual\" in df.columns and \"Total_usable_area\" in df.columns:\n",
        "        df['Area_Quality_Indicator'] = df['Total_usable_area'] * df['OverallQual']\n",
        "    else:\n",
        "        df['Area_Quality_Indicator'] = 0\n",
        "\n",
        "    if set([\"OverallQual\", \"OverallCond\", \"Total_usable_area\"]).issubset(df.columns):\n",
        "        df['Area_Qual_Cond_Indicator'] = df['Total_usable_area'] * df['OverallQual'] * df['OverallCond']\n",
        "    else:\n",
        "        df['Area_Qual_Cond_Indicator'] = 0\n",
        "\n",
        "    if \"BsmtQual\" in df.columns:\n",
        "        df['HasBsmt'] = df['BsmtQual'].notnull().astype(int)\n",
        "    else:\n",
        "        df['HasBsmt'] = 0\n",
        "\n",
        "    if set([\"OverallQual\", \"OverallCond\"]).issubset(df.columns):\n",
        "        df['Quality_conditition'] = df['OverallQual'] * df['OverallCond']\n",
        "        df['Quality_conditition_2'] = df['OverallQual'] + df['OverallCond']\n",
        "    else:\n",
        "        df['Quality_conditition'] = 0\n",
        "        df['Quality_conditition_2'] = 0\n",
        "\n",
        "    if set([\"YrSold\", \"YearRemodAdd\"]).issubset(df.columns):\n",
        "        df['House_Age2'] = df['YrSold'] - df['YearRemodAdd']\n",
        "    else:\n",
        "        df['House_Age2'] = 0\n",
        "\n",
        "    # 4) Outlier removal: only do this on TRAIN.\n",
        "    if is_train:\n",
        "        # (a) LotFrontage > 200\n",
        "        if \"LotFrontage\" in df.columns:\n",
        "            idx = df[df[\"LotFrontage\"] > 200].index\n",
        "            df.drop(idx, inplace=True)\n",
        "            y.drop(idx, inplace=True)\n",
        "\n",
        "        # (b) LotArea > 100000\n",
        "        if \"LotArea\" in df.columns:\n",
        "            idx = df[df[\"LotArea\"] > 100000].index\n",
        "            df.drop(idx, inplace=True)\n",
        "            y.drop(idx, inplace=True)\n",
        "\n",
        "        # (c) BsmtFinSF1 > 4000\n",
        "        if \"BsmtFinSF1\" in df.columns:\n",
        "            idx = df[df[\"BsmtFinSF1\"] > 4000].index\n",
        "            df.drop(idx, inplace=True)\n",
        "            y.drop(idx, inplace=True)\n",
        "\n",
        "        # (d) TotalBsmtSF > 6000\n",
        "        if \"TotalBsmtSF\" in df.columns:\n",
        "            idx = df[df[\"TotalBsmtSF\"] > 6000].index\n",
        "            df.drop(idx, inplace=True)\n",
        "            y.drop(idx, inplace=True)\n",
        "\n",
        "        # (e) 1stFlrSF > 4000\n",
        "        if \"1stFlrSF\" in df.columns:\n",
        "            idx = df[df[\"1stFlrSF\"] > 4000].index\n",
        "            df.drop(idx, inplace=True)\n",
        "            y.drop(idx, inplace=True)\n",
        "\n",
        "        # (f) GrLivArea > 4000 & y < 300000\n",
        "        if \"GrLivArea\" in df.columns:\n",
        "            idx = df[(df[\"GrLivArea\"] > 4000) & (y < 300000)].index\n",
        "            df.drop(idx, inplace=True)\n",
        "            y.drop(idx, inplace=True)\n",
        "\n",
        "        # (g) LowQualFinSF > 550\n",
        "        if \"LowQualFinSF\" in df.columns:\n",
        "            idx = df[df[\"LowQualFinSF\"] > 550].index\n",
        "            df.drop(idx, inplace=True)\n",
        "            y.drop(idx, inplace=True)\n",
        "\n",
        "    # 5) Ordinal Encoding for the columns in ordinal_mapping\n",
        "    existing_ordinal = [col for col in ordinal_cols if col in df.columns]\n",
        "\n",
        "    # Clean up whitespace if any\n",
        "    for col in existing_ordinal:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "    if len(existing_ordinal) > 0:\n",
        "        if is_train:\n",
        "            df[existing_ordinal] = ordinal_encoder.fit_transform(df[existing_ordinal])\n",
        "        else:\n",
        "            df[existing_ordinal] = ordinal_encoder.transform(df[existing_ordinal])\n",
        "        # Replace unknown encodings (-1) with 0\n",
        "        df.replace(-1, 0, inplace=True)\n",
        "\n",
        "    # 6) One-Hot Encoding for nominal features\n",
        "    existing_nominal = [col for col in nominal_features if col in df.columns]\n",
        "\n",
        "    if len(existing_nominal) > 0:\n",
        "        if is_train:\n",
        "            # Fit on train\n",
        "            encoded = one_hot_encoder.fit_transform(df[existing_nominal])\n",
        "        else:\n",
        "            # Transform test\n",
        "            encoded = one_hot_encoder.transform(df[existing_nominal])\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        encoded_df = pd.DataFrame(\n",
        "            encoded,\n",
        "            columns=one_hot_encoder.get_feature_names_out(existing_nominal),\n",
        "            index=df.index\n",
        "        )\n",
        "        # Drop original nominal cols, then join\n",
        "        df.drop(columns=existing_nominal, inplace=True)\n",
        "        df = df.join(encoded_df)\n",
        "\n",
        "    # Return final results\n",
        "    if is_train:\n",
        "        return df, y\n",
        "    else:\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTRR9um8k81l"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"../home-data-for-ml-course/train.csv\")\n",
        "X_train, y_train = preprocess_data(df_train, is_train=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11LBplRDlCPn"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(\"../home-data-for-ml-course/test.csv\")\n",
        "test_ids = df_test['Id'].copy()\n",
        "X_test = preprocess_data(df_test, is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l6o8c9alEdW",
        "outputId": "81b7106b-5538-46ba-9744-17e0e1d9b14e"
      },
      "outputs": [],
      "source": [
        "print(f\"New train shape: {X_train.shape}, test shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbd_MuPYJhRI"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heTk8IYClcM5"
      },
      "outputs": [],
      "source": [
        "# param_dist = {\n",
        "#     'n_estimators': randint(100, 3000),        # Number of trees\n",
        "# }\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': randint(100, 2000),        # Number of trees\n",
        "    'max_depth': randint(1, 20),             # Maximum tree depth\n",
        "    'min_samples_split': randint(2, 20),     # Minimum samples required to split an internal node\n",
        "    'min_samples_leaf': randint(1, 10),      # Minimum samples required at a leaf node\n",
        "    'max_features': ['sqrt', 'log2', None], # Number of features to consider at each split\n",
        "    'criterion': ['squared_error'] # For regression\n",
        "}\n",
        "#\n",
        "#param_dist_reduced = {\n",
        "#    'n_estimators': randint(100, 500),        # Use fewer trees\n",
        "#    'max_depth': randint(3, 15),              # Limit tree depth to reduce complexity\n",
        "#    'min_samples_split': randint(10, 50),     # Increase minimum samples for splitting nodes\n",
        "#    'min_samples_leaf': randint(5, 20),       # Increase minimum samples for leaves\n",
        "#    'max_features': ['sqrt', 0.5, None],      # Test 'sqrt', 50% of features, or no limit\n",
        "#    'criterion': ['squared_error']            # Stick with one criterion for consistency\n",
        "#}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho8XqF8OncdF"
      },
      "outputs": [],
      "source": [
        "# Initialize the RandomForestRegressor\n",
        "rf = RandomForestRegressor(bootstrap=True,oob_score=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9UXQEebnsXC"
      },
      "outputs": [],
      "source": [
        "# cv_dummy = [(np.arange(len(X_train)), np.arange(len(X_train)))]\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    random_state=42,\n",
        "    n_jobs=1,\n",
        "    verbose=5,\n",
        "    return_train_score=True,\n",
        "    cv=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5k3UnY9o2T-"
      },
      "outputs": [],
      "source": [
        "random_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlddKHAOpCrb"
      },
      "outputs": [],
      "source": [
        "# Get the best parameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "best_r2 = -random_search.best_score_\n",
        "print(\"Random Forest best params:\", best_params)\n",
        "print(\"Random Forest best RÂ² on validation set: {:.4f}\".format(best_r2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ght7mQ0epGpr"
      },
      "outputs": [],
      "source": [
        "# Train a final RandomForestRegressor using the best parameters\n",
        "final_model = RandomForestRegressor(**best_params,bootstrap=True,oob_score=True, random_state=42)\n",
        "final_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhhoNaBHp15e"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test set\n",
        "test_predictions = final_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9fZzL3Ep510"
      },
      "outputs": [],
      "source": [
        "# Create a submission dataframe (assuming the sample_submission.csv expects 'Id' and 'SalePrice')\n",
        "submission = pd.DataFrame({'Id': test_ids, 'SalePrice': test_predictions})\n",
        "submission.to_csv('submission_Random_forest.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission.csv' created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG6BpeFFqBWI"
      },
      "source": [
        "## XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACgUaBKYqDzC"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': list(range(1000, 1601, 50)),\n",
        "    'max_depth': randint(2, 7),\n",
        "    'learning_rate': [0.01, 0.03, 0.1, 0.3, 0.05, 0.5],\n",
        "    'subsample': [0.7, 0.6, 0.5, 0.4, 0.3],\n",
        "    'colsample_bytree': [0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "}\n",
        "# param_grid = {\n",
        "#     'n_estimators': randint(50, 1000)\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li4RdiwwqkJ4"
      },
      "outputs": [],
      "source": [
        "xgb_reg = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwHP9s4aqsLc"
      },
      "outputs": [],
      "source": [
        "# Set up RandomizedSearchCV using negative RMSE as the scoring metric\n",
        "random_search_xgb = RandomizedSearchCV(estimator=xgb_reg,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=50,\n",
        "                                   cv=4,\n",
        "                                   scoring='neg_root_mean_squared_error',\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=1,\n",
        "                                   verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7nH3h9Req3oe",
        "outputId": "1b046a86-f465-4a44-9696-93a58870dad9"
      },
      "outputs": [],
      "source": [
        "# Run the search\n",
        "random_search_xgb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqYXQejVs21v",
        "outputId": "4e009761-d346-4c2a-a744-f3f6a4504b97"
      },
      "outputs": [],
      "source": [
        "# Get the best parameters and model\n",
        "best_params_xgb = random_search_xgb.best_params_\n",
        "best_model_xgb = random_search_xgb.best_estimator_\n",
        "best_rmse_xgb = -random_search_xgb.best_score_\n",
        "print(\"XGBoost best params:\", best_params_xgb)\n",
        "print(\"XGBosst best RMSE on validation set: {:.4f}\".format(best_rmse_xgb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqDHrxO8s6s0"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the validation set\n",
        "# val_predictions = best_model.predict(X_val)\n",
        "# rmse_val = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
        "# print(\"Best Parameters:\", best_params)\n",
        "# print(\"Validation RMSE:\", rmse_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "wU0dqo9Ttj6d",
        "outputId": "97ae526b-a638-4f3c-c5e6-8de4f9893fba"
      },
      "outputs": [],
      "source": [
        "# Train the final XGBRegressor using the best parameters\n",
        "final_model_xgb = xgb.XGBRegressor(**best_params_xgb, objective='reg:squarederror', random_state=42)\n",
        "final_model_xgb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fb3yPIptUCA"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test set\n",
        "test_predictions_xgb = final_model_xgb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMyq_h0ktuaw",
        "outputId": "13dd5de8-b754-4127-a278-859542a41075"
      },
      "outputs": [],
      "source": [
        "# Create a submission dataframe (assumes sample_submission.csv expects 'Id' and 'SalePrice')\n",
        "submission = pd.DataFrame({'Id': test_ids, 'SalePrice': test_predictions_xgb})\n",
        "submission.to_csv('submission_XGBoost.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission_XGBoost.csv' created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVRL9pCAwjcA"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOLNf4VCwmJG"
      },
      "outputs": [],
      "source": [
        "param_dist_ridge = {\n",
        "    'alpha': uniform(0.01, 100)   # Sample alpha between 0.01 and 100\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNzvky7Qwpa8"
      },
      "outputs": [],
      "source": [
        "ridge = Ridge()\n",
        "\n",
        "random_search_ridge = RandomizedSearchCV(\n",
        "    estimator=ridge,\n",
        "    param_distributions=param_dist_ridge,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    random_state=42,\n",
        "    n_jobs=1,\n",
        "    verbose=2,\n",
        "    return_train_score=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rDZR23E_wzVf",
        "outputId": "20490d09-34af-4d03-928d-3ef49bad2ea6"
      },
      "outputs": [],
      "source": [
        "random_search_ridge.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WvWxscuxBK5",
        "outputId": "26f08602-8175-441d-d31a-3daf5b423d54"
      },
      "outputs": [],
      "source": [
        "# Best hyperparameters and score (convert negative RMSE to RMSE)\n",
        "best_params_ridge = random_search_ridge.best_params_\n",
        "best_rmse_ridge = -random_search_ridge.best_score_\n",
        "print(\"Ridge Regression best params:\", best_params_ridge)\n",
        "print(\"Ridge Regression best RMSE on validation set: {:.4f}\".format(best_rmse_ridge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "HpQPl7tNxFpr",
        "outputId": "1d151fa6-8b95-434c-91e9-7cee7b5db265"
      },
      "outputs": [],
      "source": [
        "# Train final model on the original training set using best hyperparameters\n",
        "final_ridge = Ridge(**best_params_ridge)\n",
        "final_ridge.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVeidt_6aqiD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15V7c8PtxZgW"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test set\n",
        "test_predictions_ridge = final_ridge.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWX4_BhzxlJu",
        "outputId": "68e6348f-bcd6-42a3-dc24-2cbcf314e30f"
      },
      "outputs": [],
      "source": [
        "# Create a submission dataframe (assumes sample_submission.csv expects 'Id' and 'SalePrice')\n",
        "submission = pd.DataFrame({'Id': test_ids, 'SalePrice': test_predictions_ridge})\n",
        "submission.to_csv('submission_ridge.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission_ridge.csv' created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ben0etMTxtWR"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN65Ldt3xv0D"
      },
      "outputs": [],
      "source": [
        "#param_dist_svr = {\n",
        "#    'C': uniform(0.1, 10),\n",
        "#    'epsilon': uniform(0.01, 1),\n",
        "#    'kernel': ['rbf', 'linear']\n",
        "#}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhCApl7PDNPE"
      },
      "outputs": [],
      "source": [
        "param_grid_svr = {\n",
        "    'C': [0.1, 1, 3],\n",
        "    'epsilon': [0.01, 0.1, 0.3],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9VB01lqx2u1"
      },
      "outputs": [],
      "source": [
        "svr = SVR()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-RK41PIx5d5"
      },
      "outputs": [],
      "source": [
        "# random_search_svr = RandomizedSearchCV(\n",
        "#    estimator=svr,\n",
        "#    param_distributions=param_dist_svr,\n",
        "#    n_iter=50,\n",
        "#    cv=5,\n",
        "#    scoring='neg_root_mean_squared_error',\n",
        "#    random_state=42,\n",
        "#    n_jobs=-1,\n",
        "#    verbose=2,\n",
        "#    return_train_score=True\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk2DkC00DRRA"
      },
      "outputs": [],
      "source": [
        "\n",
        "grid_search_svr = GridSearchCV(\n",
        "    estimator=svr,\n",
        "    param_grid=param_grid_svr,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_nxgp5YvDdo2",
        "outputId": "2eb6b198-8a4c-443a-bfaf-313c27877762"
      },
      "outputs": [],
      "source": [
        "grid_search_svr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5Mn0gmCqx8x2"
      },
      "outputs": [],
      "source": [
        "# Fit the randomized search on the combined dataset\n",
        "# random_search_svr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKHNLSEuyA71"
      },
      "outputs": [],
      "source": [
        "# best_params_svr = random_search_svr.best_params_\n",
        "# best_rmse_svr = -random_search_svr.best_score_\n",
        "# print(\"SVR best params:\", best_params_svr)\n",
        "# print(\"SVR best RMSE on validation set: {:.4f}\".format(best_rmse_svr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OpRaR9EDp8I"
      },
      "outputs": [],
      "source": [
        "best_params_svr = grid_search_svr.best_params_\n",
        "best_rmse_svr = -grid_search_svr.best_score_\n",
        "print(\"Best Parameters:\", best_params_svr)\n",
        "print(\"Best RMSE:\", best_rmse_svr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or6XgPcayEjV"
      },
      "outputs": [],
      "source": [
        "final_svr = SVR(**best_params_svr)\n",
        "final_svr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2EifOUPyIf8"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test set\n",
        "test_predictions_svr = final_svr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5JfLF7jyPiN"
      },
      "outputs": [],
      "source": [
        "\n",
        "submission = pd.DataFrame({'Id': test_ids, 'SalePrice': test_predictions_svr})\n",
        "submission.to_csv('submission_svr.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission_svr.csv' created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIsOkzgY5eKz"
      },
      "source": [
        "## Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5izRK8w65gM-"
      },
      "outputs": [],
      "source": [
        "best_models = {\n",
        "    'Random Forest': random_search.best_estimator_,\n",
        "    'XGBoost': random_search_xgb.best_estimator_,\n",
        "    'Linear Regression': random_search_ridge.best_estimator_,\n",
        "    'SVM': grid_search_svr.best_estimator_\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43kgcrM36FHe"
      },
      "outputs": [],
      "source": [
        "# Prepare a list to store results\n",
        "results = []\n",
        "\n",
        "# Evaluate each model on the validation set\n",
        "for model_name, model in best_models.items():\n",
        "    y_pred = model.predict(X_val)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    # Convert the best params dictionary to a formatted string for display\n",
        "    best_params = model.get_params()\n",
        "    best_params_str = \", \".join([f\"{k}={v}\" for k, v in best_params.items() if k in best_models[model_name].get_params()])\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'RMSE': rmse,\n",
        "        'Best Parameters': best_params_str\n",
        "    })\n",
        "\n",
        "# Create a DataFrame to display the results in a table format\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbSfMONhCTiS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hqCZxB5CRtR"
      },
      "source": [
        "## Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNJIFwkECUMq"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Random Forest\": (random_search.best_estimator_, random_search.best_params_),\n",
        "    \"XGBoost\": (random_search_xgb.best_estimator_, random_search_xgb.best_params_),\n",
        "    \"Linear Regression\": (random_search_ridge.best_estimator_, random_search_ridge.best_params_),\n",
        "    \"SVM\": (grid_search_svr.best_estimator_, grid_search_svr.best_params_)\n",
        "}\n",
        "\n",
        "# Prepare a list to store the evaluation results\n",
        "results = []\n",
        "\n",
        "# Evaluate each model on the validation set\n",
        "for model_name, (model, best_params) in models.items():\n",
        "    # Predict on the validation set\n",
        "    y_pred = model.predict(X_val)\n",
        "    # Compute RMSE (ensure it's positive)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    # Append the model results, including the best parameters dictionary\n",
        "    results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"RMSE\": rmse,\n",
        "        \"Best Parameters\": best_params\n",
        "    })\n",
        "\n",
        "# Create a DataFrame to display the results in a neat table\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
